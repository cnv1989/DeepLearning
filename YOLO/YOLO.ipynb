{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Object detection using YOLO</h1>\n",
    "\n",
    "<p>In this tutorial we will implement object detection using pretrained YOLO model.</p>\n",
    "\n",
    "<h3>Step 1: Download pretrained YOLO model and weights</h3>\n",
    "\n",
    "<b>Get Weights</b><br/>\n",
    "<code>wget http://pjreddie.com/media/files/yolo.weights</code>\n",
    "\n",
    "<b>Get Model Configuration</b><br/>\n",
    "<code>wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolo.cfg</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Convert YOLO configuration to JSON</h2>\n",
    "\n",
    "<p>To simplify building the model from the cfg file we will first convert the configureation into python dictionary</p>\n",
    "\n",
    "<p>Lets explore the configuration file a litte and analyze what each of the sections mean. There are 6 types of sections in the configuration file</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>net:</b> Contains hyperparameters and the input shape of model.\n",
    "    </li>\n",
    "    <li><b>convolutional:</b> Simple convolutional layer.</li>\n",
    "    <li><b>maxpool:</b> Simple maxpool layer.</li>\n",
    "    <li>\n",
    "        <b>route:</b> This generates a shortcut by concatenating previous layers (inception).\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>reorg:</b> This operation moves elements from a channel into filter. For ex. a single channel 2x2 block will be rearranged into 1x1x4 block.\n",
    "    </li>\n",
    "    <li><b>region:</b> Contains hyperparameters from box filtering and non-max supression.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_filepath = \"./yolo.cfg\"\n",
    "\n",
    "BYTE_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYS = [\"net\", \"convolutional\", \"maxpool\", \"route\", \"reorg\", \"region\"]\n",
    "\n",
    "def convert_config_file_to_json(path):\n",
    "    config_file = open(path)\n",
    "    data = []\n",
    "    \n",
    "    def is_comment(line):\n",
    "        return line.startswith(\"#\")\n",
    "    \n",
    "    block = {}\n",
    "    for line in config_file:\n",
    "        line = line.strip()\n",
    "        if line and not is_comment(line):\n",
    "            if line.strip(\"[\").strip(\"]\") in KEYS:\n",
    "                if block:\n",
    "                    data.append(block)\n",
    "                    \n",
    "                block = {\n",
    "                    \"layer\": line.strip(\"[\").strip(\"]\")\n",
    "                }\n",
    "            else:\n",
    "                key, val = line.split(\"=\")\n",
    "                block[key.strip()] = val.strip()\n",
    "                \n",
    "    if block:\n",
    "        data.append(block)\n",
    "                \n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Lets build YOLO model in Keras</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nag/.virtualenvs/deep_learning/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def convolutional_block(X_IN, info, parameters):\n",
    "    weights = get_conv_weights(info, X_IN)\n",
    "    batch_normalize = info.get('batch_normalize')\n",
    "    filters = int(info.get('filters'))\n",
    "    size = int(info.get('size'))\n",
    "    stride = int(info.get('stride'))\n",
    "    X = Conv2D(\n",
    "        filters, \n",
    "        (size, size),\n",
    "        kernel_regularizer=l2(float(parameters.get('decay'))),\n",
    "        use_bias=not batch_normalize,\n",
    "        strides=(stride, stride),\n",
    "        weights=weights[0],\n",
    "        padding='same' if int(info.get('pad')) == 1 else 'valid'\n",
    "    )(X_IN)\n",
    "\n",
    "    if info.get('batch_normalize'):\n",
    "        X = BatchNormalization(weights=weights[1])(X)\n",
    "    \n",
    "    if info.get('activation') == 'leaky':\n",
    "        X = LeakyReLU(alpha=0.1)(X)\n",
    "    return X\n",
    "\n",
    "def maxpool_block(X_IN, info):\n",
    "    size = int(info.get('size'))\n",
    "    stride = int(info.get('stride'))\n",
    "    return MaxPooling2D(\n",
    "        pool_size=(size, size),\n",
    "        strides=(stride, stride),\n",
    "        padding='same'\n",
    "    )(X_IN)\n",
    "\n",
    "\n",
    "def space_to_depth_x2(x):\n",
    "    \"\"\"Thin wrapper for Tensorflow space_to_depth with block_size=2.\"\"\"\n",
    "    # Import currently required to make Lambda work.\n",
    "    # See: https://github.com/fchollet/keras/issues/5088#issuecomment-273851273\n",
    "    import tensorflow as tf\n",
    "    return tf.space_to_depth(x, block_size=2)\n",
    "\n",
    "def reorg_block(X_IN, info):\n",
    "    return Lambda(space_to_depth_x2)(X_IN)\n",
    "\n",
    "\n",
    "def get_conv_weights(layer, prev_layer):\n",
    "    size = int(layer[\"size\"])\n",
    "    filters = int(layer[\"filters\"])\n",
    "    channels = prev_layer.shape[-1]\n",
    "    weights_shape = (size, size, int(channels), filters)\n",
    "    darknet_w_shape = (filters, int(channels), size, size)  # weights_shape.reverse()\n",
    "\n",
    "    # number of bias term of a layer = number of filters\n",
    "    conv_bias = np.ndarray(\n",
    "        shape=(filters, ), \n",
    "        dtype='float32', \n",
    "        buffer=weights_file.read(BYTE_SIZE*filters))\n",
    "\n",
    "    bn_weight_list = None\n",
    "    \n",
    "    if layer.get('batch_normalize'):\n",
    "        # (gama, beta and epsilon) per filter\n",
    "        bn_weights = np.ndarray(\n",
    "            shape=(3, filters),\n",
    "            dtype='float32',\n",
    "            buffer=weights_file.read(BYTE_SIZE*3*filters)\n",
    "        )\n",
    "        bn_weight_list = [\n",
    "            bn_weights[0],  # scale gamma\n",
    "            conv_bias,  # shift beta\n",
    "            bn_weights[1],  # running mean\n",
    "            bn_weights[2]  # running var\n",
    "        ]\n",
    "\n",
    "    conv_weights = np.ndarray(\n",
    "        shape=darknet_w_shape,\n",
    "        dtype='float32',\n",
    "        buffer=weights_file.read(BYTE_SIZE*np.product(darknet_w_shape))\n",
    "    )\n",
    "    \n",
    "    conv_weights = np.transpose(conv_weights, [2, 3, 1, 0])\n",
    "    \n",
    "    conv_weights = [conv_weights] if layer.get('batch_normalize') else [conv_weights, conv_bias]\n",
    "    \n",
    "    return (conv_weights, bn_weight_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "weights_filepath = \"./yolo.weights\"\n",
    "weights_file = open(weights_filepath, 'rb')\n",
    "\n",
    "layers_data = convert_config_file_to_json(config_filepath)\n",
    "parameters = layers_data[0]\n",
    "\n",
    "image_height = int(parameters['height'])\n",
    "image_width = int(parameters['width'])\n",
    "channels = int(parameters[\"channels\"])\n",
    "\n",
    "all_layers = [Input(shape=(image_height, image_width, channels))]\n",
    "\n",
    "weights_header = np.ndarray(shape=(4, ), dtype='int32', buffer=weights_file.read(4*BYTE_SIZE))\n",
    "\n",
    "for layer_info in layers_data:\n",
    "    prev_layer = all_layers[-1]\n",
    "    if layer_info[\"layer\"] == \"convolutional\":\n",
    "        layer = convolutional_block(prev_layer, layer_info, parameters)\n",
    "        all_layers.append(layer)\n",
    "    elif layer_info[\"layer\"] == \"maxpool\":\n",
    "        layer = maxpool_block(prev_layer, layer_info)\n",
    "        all_layers.append(layer)\n",
    "    elif layer_info[\"layer\"] == \"route\":\n",
    "        ids = [int(i) for i in layer_info['layers'].split(',')]\n",
    "        concat_layers = [all_layers[i] for i in ids]\n",
    "        if len(concat_layers) > 1:\n",
    "            all_layers.append(concatenate(concat_layers))\n",
    "        else:\n",
    "            all_layers.append(concat_layers[0])\n",
    "    elif layer_info[\"layer\"] == \"reorg\":\n",
    "        layer = reorg_block(prev_layer, layer_info)\n",
    "        all_layers.append(layer)\n",
    "        \n",
    "\n",
    "remaining_weights = len(weights_file.read()) / BYTE_SIZE\n",
    "\n",
    "assert remaining_weights == 0, \"There are remaining weights.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 608, 608, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 608, 608, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 608, 608, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 608, 608, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 304, 304, 32) 0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 304, 304, 64) 18432       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 304, 304, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 304, 304, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 152, 152, 64) 0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 152, 152, 128 73728       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 152, 152, 128 512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 152, 152, 128 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 152, 152, 64) 8192        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 152, 152, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 152, 152, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 152, 152, 128 73728       leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 152, 152, 128 512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 152, 152, 128 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 76, 76, 128)  0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 76, 76, 256)  294912      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 76, 76, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 76, 76, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 76, 76, 128)  32768       leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 76, 76, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 76, 76, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 76, 76, 256)  294912      leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 76, 76, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 76, 76, 256)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 38, 38, 256)  0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 38, 38, 512)  1179648     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 38, 38, 512)  2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 38, 38, 512)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 38, 38, 256)  131072      leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 38, 38, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 38, 38, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 38, 38, 512)  1179648     leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 38, 38, 512)  2048        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 38, 38, 512)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 38, 38, 256)  131072      leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 38, 38, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 38, 38, 256)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 38, 38, 512)  1179648     leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 38, 38, 512)  2048        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 38, 38, 512)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 19, 19, 512)  0           leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 19, 19, 1024) 4718592     max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 19, 19, 1024) 4096        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 19, 19, 512)  524288      leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 19, 19, 512)  2048        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 19, 19, 512)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 19, 19, 1024) 4718592     leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 19, 19, 1024) 4096        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 19, 19, 512)  524288      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 19, 19, 512)  2048        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 19, 19, 512)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 19, 19, 1024) 4718592     leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 19, 19, 1024) 4096        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 19, 19, 1024) 9437184     leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 19, 19, 1024) 4096        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 38, 38, 64)   32768       leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 38, 38, 64)   256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 19, 19, 1024) 9437184     leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 38, 38, 64)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 19, 19, 1024) 4096        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 19, 19, 256)  0           leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 19, 19, 1280) 0           lambda_1[0][0]                   \n",
      "                                                                 leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 19, 19, 1024) 11796480    concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 19, 19, 1024) 4096        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 19, 19, 425)  435625      leaky_re_lu_22[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 50,983,561\n",
      "Trainable params: 50,962,889\n",
      "Non-trainable params: 20,672\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model = Model(inputs=all_layers[0], outputs=all_layers[-1])\n",
    "print(model.summary())\n",
    "\n",
    "model.save(\"yolo.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
