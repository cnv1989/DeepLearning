{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Object detection using YOLO</h1>\n",
    "\n",
    "<p>In this tutorial we will implement object detection using pretrained YOLO model.</p>\n",
    "\n",
    "<h3>Step 1: Download pretrained YOLO model and weights</h3>\n",
    "\n",
    "<b>Get Weights</b><br/>\n",
    "<code>wget http://pjreddie.com/media/files/yolo.weights</code>\n",
    "\n",
    "<b>Get Model Configuration</b><br/>\n",
    "<code>wget https://github.com/pjreddie/darknet/blob/master/cfg/yolo-voc.cfg</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Convert YOLO configuration to JSON</h2>\n",
    "\n",
    "<p>To simplify building the model from the cfg file we will first convert the configureation into python dictionary</p>\n",
    "\n",
    "<p>Lets explore the configuration file a litte and analyze what each of the sections mean. There are 6 types of sections in the configuration file</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>net:</b> Contains hyperparameters and the input shape of model.\n",
    "    </li>\n",
    "    <li><b>convolutional:</b> Simple convolutional layer.</li>\n",
    "    <li><b>maxpool:</b> Simple maxpool layer.</li>\n",
    "    <li>\n",
    "        <b>route:</b> This generates a shortcut by concatenating previous layers (inception).\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>reorg:</b> This operation moves elements from a channel into filter. For ex. a single channel 2x2 block will be rearranged into 1x1x4 block.\n",
    "    </li>\n",
    "    <li><b>region:</b> Contains hyperparameters from box filtering and non-max supression.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_filepath = \"./yolo.cfg\"\n",
    "\n",
    "BYTE_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYS = [\"net\", \"convolutional\", \"maxpool\", \"route\", \"reorg\", \"region\"]\n",
    "\n",
    "def convert_config_file_to_json(path):\n",
    "    config_file = open(path)\n",
    "    data = []\n",
    "    \n",
    "    def is_comment(line):\n",
    "        return line.startswith(\"#\")\n",
    "    \n",
    "    block = {}\n",
    "    for line in config_file:\n",
    "        line = line.strip()\n",
    "        if line and not is_comment(line):\n",
    "            if line.strip(\"[\").strip(\"]\") in KEYS:\n",
    "                if block:\n",
    "                    data.append(block)\n",
    "                    \n",
    "                block = {\n",
    "                    \"layer\": line.strip(\"[\").strip(\"]\")\n",
    "                }\n",
    "            else:\n",
    "                key, val = line.split(\"=\")\n",
    "                block[key.strip()] = val.strip()\n",
    "                \n",
    "    if block:\n",
    "        data.append(block)\n",
    "                \n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Let's write methods to generate model without weights</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nag/.virtualenvs/deep_learning/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def convolutional_block(X_IN, info, parameters):\n",
    "    weights = get_conv_weights(info, X_IN)\n",
    "    batch_normalize = info.get('batch_normalize')\n",
    "    filters = int(info.get('filters'))\n",
    "    size = int(info.get('size'))\n",
    "    stride = int(info.get('stride'))\n",
    "    X = Conv2D(\n",
    "        filters, \n",
    "        (size, size),\n",
    "        kernel_regularizer=l2(float(parameters.get('decay'))),\n",
    "        use_bias=not batch_normalize,\n",
    "        strides=(stride, stride),\n",
    "        weights=weights[0],\n",
    "        padding='same' if int(info.get('pad')) == 1 else 'valid'\n",
    "    )(X_IN)\n",
    "\n",
    "    if info.get('batch_normalize'):\n",
    "        X = BatchNormalization(weights=weights[1])(X)\n",
    "    \n",
    "    if info.get('activation') == 'leaky':\n",
    "        X = LeakyReLU(alpha=0.01)(X)\n",
    "    return X\n",
    "\n",
    "def maxpool_block(X_IN, info):\n",
    "    size = int(info.get('size'))\n",
    "    stride = int(info.get('stride'))\n",
    "    return MaxPooling2D(\n",
    "        pool_size=(size, size),\n",
    "        strides=(stride, stride),\n",
    "        padding='same'\n",
    "    )(X_IN)\n",
    "\n",
    "def reorg_block(X_IN, info):\n",
    "    return Lambda(lambda x: tf.space_to_depth(x, block_size=int(info.get('stride', 2))))(X_IN)\n",
    "\n",
    "\n",
    "def get_conv_weights(layer, prev_layer):\n",
    "    size = int(layer[\"size\"])\n",
    "    filters = int(layer[\"filters\"])\n",
    "    channels = prev_layer.shape[-1]\n",
    "    weights_shape = (size, size, int(channels), filters)\n",
    "    darknet_w_shape = (filters, int(channels), size, size)  # weights_shape.reverse()\n",
    "\n",
    "    # number of bias term of a layer = number of filters\n",
    "    conv_bias = np.ndarray(\n",
    "        shape=(filters, ), \n",
    "        dtype='float32', \n",
    "        buffer=weights_file.read(BYTE_SIZE*filters))\n",
    "\n",
    "    bn_weight_list = None\n",
    "    \n",
    "    if layer.get('batch_normalize'):\n",
    "        # (gama, beta and epsilon) per filter\n",
    "        bn_weights = np.ndarray(\n",
    "            shape=(3, filters),\n",
    "            dtype='float32',\n",
    "            buffer=weights_file.read(BYTE_SIZE*3*filters)\n",
    "        )\n",
    "        bn_weight_list = [\n",
    "            bn_weights[0],  # scale gamma\n",
    "            conv_bias,  # shift beta\n",
    "            bn_weights[1],  # running mean\n",
    "            bn_weights[2]  # running var\n",
    "        ]\n",
    "\n",
    "    conv_weights = np.ndarray(\n",
    "        shape=darknet_w_shape,\n",
    "        dtype='float32',\n",
    "        buffer=weights_file.read(BYTE_SIZE*np.product(darknet_w_shape))\n",
    "    )\n",
    "    \n",
    "    conv_weights = np.transpose(conv_weights, [2, 3, 1, 0])\n",
    "    \n",
    "    conv_weights = [conv_weights] if layer.get('batch_normalize') else [conv_weights, conv_bias]\n",
    "    \n",
    "    return (conv_weights, bn_weight_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "weights_filepath = \"./yolo.weights\"\n",
    "weights_file = open(weights_filepath, 'rb')\n",
    "\n",
    "layers_data = convert_config_file_to_json(config_filepath)\n",
    "parameters = layers_data[0]\n",
    "\n",
    "image_height = int(parameters['height'])\n",
    "image_width = int(parameters['width'])\n",
    "channels = int(parameters[\"channels\"])\n",
    "\n",
    "layers = [Input(shape=(image_height, image_width, channels))]\n",
    "\n",
    "weights_header = np.ndarray(shape=(4, ), dtype='int32', buffer=weights_file.read(4*BYTE_SIZE))\n",
    "\n",
    "for layer_info in layers_data:\n",
    "    prev_layer = layers[-1]\n",
    "    if layer_info[\"layer\"] == \"convolutional\":\n",
    "        layer = convolutional_block(prev_layer, layer_info, parameters)\n",
    "        layers.append(layer)\n",
    "    elif layer_info[\"layer\"] == \"maxpool\":\n",
    "        layer = maxpool_block(prev_layer, layer_info)\n",
    "        layers.append(layer)\n",
    "    elif layer_info[\"layer\"] == \"route\":\n",
    "        ids = [int(i) for i in layer_info['layers'].split(',')]\n",
    "        concat_layers = [layers[i] for i in ids]\n",
    "        if len(concat_layers) > 1:\n",
    "            layers.append(concatenate(concat_layers))\n",
    "        else:\n",
    "            layers.append(concat_layers[0])\n",
    "    elif layer_info[\"layer\"] == \"reorg\":\n",
    "        layer = reorg_block(prev_layer, layer_info)\n",
    "        layers.append(layer)\n",
    "        \n",
    "\n",
    "remaining_weights = len(weights_file.read()) / BYTE_SIZE\n",
    "\n",
    "assert remaining_weights == 0, \"There are remaining weights.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model = Model(inputs=layers[0], outputs=layers[-1])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
